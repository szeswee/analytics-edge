{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- need to select appropriate features. right now, all columns are used as features\n",
    "- try using nested mixed logit model\n",
    "- stratified sampling for caret CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(mlogit)\n",
    "library(nnet)\n",
    "library(glmnet)\n",
    "library(randomForest)\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(xgboost)\n",
    "library(plyr)\n",
    "library(dplyr)\n",
    "library(methods)\n",
    "library(data.table)\n",
    "library(magrittr)\n",
    "library(LiblineaR)\n",
    "library(caret)\n",
    "library(parallel)\n",
    "library(doParallel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run all model fitting and prediction here and output results to file (long computation time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## place all model fit/predict wrapper functions here\n",
    "models <- c(mnlogit, cart, rf)\n",
    "\n",
    "# enable metamodel stacking??\n",
    "isStacked <- T\n",
    "stacked <- data.frame()\n",
    "probs <- data.frame()\n",
    "\n",
    "\n",
    "# run ALL models in list\n",
    "# NOTE: this loop will take a long ass time even with 36 cores\n",
    "for (m in 1:length(models)) {\n",
    "    \n",
    "    # run model fitting and prediction, and extract predicted probabilities\n",
    "    # if model stacking enabled, get classes. Else, get probabilities\n",
    "    pred <- models[[m]](prob=(!isStacked))\n",
    "\n",
    "    # metamodel stacking\n",
    "    if (isStacked) {\n",
    "        stacked <- cbind.all(stacked, pred)\n",
    "        colnames(stacked)[m] <- paste(\"M\", m, sep=\"\")\n",
    "    } else {\n",
    "        probs <- cbind.all(probs, pred)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# prepare for metamodeling if stacking is enabled\n",
    "if (isStacked) {\n",
    "        stacked <- cbind.all(stacked, actual.train)\n",
    "        colnames(stacked)[NCOL(stacked)] <- \"actualChoice\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if (isStacked) {\n",
    "#     stacked\n",
    "# } else {\n",
    "#     probs\n",
    "# }\n",
    "\n",
    "# preprocessing all predictions \n",
    "stackedDF <- as.data.frame(stacked)\n",
    "str(stackedDF)\n",
    "\n",
    "## do model averaging\n",
    "MM <- multinom(actualChoice ~ M1+M2+M3, data = stackedDF)\n",
    "p <- predict(MM, stackedDF)\n",
    "stackedDF$pred <- p\n",
    "\n",
    "# get percent accuracy according to model used\n",
    "NROW(stackedDF[stackedDF$actualChoice == stackedDF$M1, ]) # mlogit\n",
    "NROW(stackedDF[stackedDF$actualChoice == stackedDF$M2, ]) # cart\n",
    "NROW(stackedDF[stackedDF$actualChoice == stackedDF$M3, ]) # rf\n",
    "NROW(stackedDF[stackedDF$actualChoice == stackedDF$pred, ]) # stacked model\n",
    "\n",
    "stackedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "\n",
    "## place all helper functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fn to append to empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function to calculate multinomial logloss\n",
    "logLoss <- function(pred, actual){\n",
    "  -1*mean(log(pred[model.matrix(~ actual + 0) - pred > 0]))\n",
    "}\n",
    "\n",
    "# CBIND TO DF\n",
    "cbind.all <- function (...) {\n",
    "    nm <- list(...)\n",
    "    nm <- lapply(nm, as.matrix)\n",
    "    n <- max(sapply(nm, nrow))\n",
    "    do.call(cbind, lapply(nm, function(x) rbind(x, matrix(, n - \n",
    "        nrow(x), ncol(x)))))\n",
    "}\n",
    "\n",
    "# permutate list of incrementing integers\n",
    "perm <- function(n) {\n",
    "    if(n==1){\n",
    "        return(matrix(1))\n",
    "    } else {\n",
    "        sp <- perm(n-1)\n",
    "        p <- nrow(sp)\n",
    "        A <- matrix(nrow=n*p,ncol=n)\n",
    "        for(i in 1:n){\n",
    "            A[(i-1)*p+1:p,] <- cbind(i,sp+(sp>=i))\n",
    "        }\n",
    "        return(A)\n",
    "    }\n",
    "} \n",
    "\n",
    "\n",
    "# permutate for all datasets\n",
    "gen2 <- function(data) {\n",
    "\n",
    "    names <- colnames(data)\n",
    "    p <- perm(3)\n",
    "    p <- p[2:NROW(p),]\n",
    "    results <- data.frame()\n",
    "\n",
    "    spl <- list()\n",
    "    spl$alts <- list()\n",
    "    spl$alts$a <- data[,c(4:23)]\n",
    "    spl$alts$b <- data[,c(24:43)]\n",
    "    spl$alts$c <- data[,c(44:63)]\n",
    "    spl$ch <- list()\n",
    "    spl$ch$a <- data[,c(95)]\n",
    "    spl$ch$b <- data[,c(96)]\n",
    "    spl$ch$c <- data[,c(97)]\n",
    "    spl$Choice <- ifelse(data[,c(99)]==\"Ch1\", 1,\n",
    "                        ifelse(data[,c(99)]==\"Ch2\", 2,\n",
    "                                ifelse(data[,c(99)]==\"Ch3\", 3, 4)))\n",
    "    for (i in 1:NROW(p)) {\n",
    "        df <- data.frame(data[,c(1:3)],\n",
    "                        spl$alts[[p[i,1]]], spl$alts[[p[i,2]]], spl$alts[[p[i,3]]], data[,c(64:83)],\n",
    "                        data[,c(84:94)],\n",
    "                        spl$ch[[p[i,1]]], spl$ch[[p[i,2]]], spl$ch[[p[i,3]]], data[,c(98)],\n",
    "                        ifelse(spl$Choice!=4, paste(\"Ch\", match(spl$Choice, p[i,]), sep=\"\"), \"Ch4\"))\n",
    "        colnames(df) <- names\n",
    "        results <- rbind(results, df)\n",
    "    }\n",
    "    \n",
    "    return(results)\n",
    "}\n",
    "\n",
    "# # NROW(data.train.small)\n",
    "# haha2 <- gen(data.train.small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import training set\n",
    "data.train <- subset(read.csv(\"csv/train.csv\"), Task<=19)\n",
    "# data.train.small <- subset(read.csv(\"csv/train.csv\"), Task<=12)\n",
    "data.train.small <- subset(read.csv(\"csv/train.csv\"), Case<=600)\n",
    "# import validation set\n",
    "# data.valid <- subset(read.csv(\"csv/train.csv\"), Task>=13)\n",
    "data.valid <- subset(read.csv(\"csv/train.csv\"), Case>600)\n",
    "# import test set\n",
    "data.test <- read.csv(\"csv/test.csv\")\n",
    "\n",
    "# releveling `income` factor levels to be consistent throughout\n",
    "fac.income <- union(levels(data.test$income), levels(data.train$income))\n",
    "data.train$income <- factor(data.train$income, levels = fac.income)\n",
    "data.train.small$income <- factor(data.train.small$income, levels = fac.income)\n",
    "data.valid$income <- factor(data.valid$income, levels = fac.income)\n",
    "data.test$income <- factor(data.test$income, levels = fac.income)\n",
    "\n",
    "# adding values to `Ch` andd `Choice` levels to prevent errors\n",
    "data.test[,c(\"Ch1\", \"Ch2\", \"Ch3\", \"Ch4\")] <- 1\n",
    "data.test[,c(\"Choice\")] <- \"Ch1\"\n",
    "# data.train[,c(4:83)] <- data.train[,c(4:83)] + 1\n",
    "# data.valid[,c(4:83)] <- data.valid[,c(4:83)] + 1\n",
    "# data.test[,c(4:83)] <- data.test[,c(4:83)] + 1\n",
    "\n",
    "# bootstrapping\n",
    "data.train.mod <- rbind(data.train, gen2(data.train))\n",
    "data.train.small.mod <- rbind(data.train.small, gen2(data.train.small))\n",
    "# data.valid <- rbind(data.valid, gen(data.valid))\n",
    "\n",
    "\n",
    "\n",
    "# reordering each alternative from most expensive to least expensive\n",
    "# ties are broken by placing those with least features first\n",
    "mapping <- function(df) {\n",
    "#     for (r in 1:22) {\n",
    "    for (r in 1:NROW(df)) {\n",
    "        alts <- list()\n",
    "        alts$A1 <- df[r,c(4:22)]\n",
    "        alts$A2 <- df[r,c(24:42)]\n",
    "        alts$A3 <- df[r,c(44:62)]\n",
    "        costs <- c(df[r,c(23)], df[r,c(43)], df[r,c(63)])\n",
    "        ch <- c(df[r,c(95)], df[r,c(96)], df[r,c(97)])\n",
    "        Choice <- df[r,c(99)]\n",
    "        map <- rank(costs, ties=\"first\")\n",
    "#         cat(\"map: \", map, \"\\n\")\n",
    "        inv.map <- rank(map, ties=\"first\")\n",
    "#         cat(\"inv: \", inv.map, \"\\n\")\n",
    "\n",
    "\n",
    "        # extract orders\n",
    "        for (m in 1:3) {\n",
    "            rk.new <- match(m, map)\n",
    "            df[r,c(((m-1)*20+4):((m-1)*20+22))] <- alts[[rk.new]] # all car safety features\n",
    "            df[r,c(23+20*(m-1))] <- costs[rk.new] # all alternative prices\n",
    "            df[r,c(95+m-1)] <- ch[rk.new] # all binary choices\n",
    "        }\n",
    "        if (!is.na(df[r,c(99)])) {\n",
    "            Choice.new <- df[r,c(99)] %>% gsub('Ch', '', .) %>% {as.integer(.)} #factorial choice\n",
    "            if (Choice.new != 4) {\n",
    "                df[r,c(99)] <- paste(\"Ch\", map[Choice.new], sep=\"\")\n",
    "            }\n",
    "        }\n",
    "#         first <- match(1, map)\n",
    "#         first.alt <- alts[[first]]\n",
    "#         first.cost <- costs[first]\n",
    "#         second <- match(2, map)\n",
    "#         second.alt <- alts[[second]]\n",
    "#         second.cost <- costs[second]\n",
    "#         third <- match(3, map)\n",
    "#         third.alt <- alts[[third]]\n",
    "#         third.cost <- costs[third]\n",
    "\n",
    "        # inputtting into dataframe\n",
    "#         df[r,c(4:22)] <- first.alt\n",
    "#         df[r,c(23)] <- first.cost\n",
    "#         df[r,c(24:42)] <- second.alt\n",
    "#         df[r,c(43)] <- second.cost\n",
    "#         df[r,c(44:62)] <- third.alt\n",
    "#         df[r,c(63)] <- third.cost\n",
    "\n",
    "        # appending to output\n",
    "#         out <- list()\n",
    "#         out$map <- map\n",
    "#         out$invmap <- inv.map\n",
    "#         out$df <- df\n",
    "    }\n",
    "    return(df)\n",
    "}\n",
    "# data.train.order <- mapping(data.train)\n",
    "# data.valid.order <- mapping(data.valid)\n",
    "# data.test.order <- mapping(data.test)\n",
    "\n",
    "\n",
    "# get actual choices\n",
    "actual.train <- data.train[,\"Choice\"]\n",
    "actual.train.small <- data.train.small[,\"Choice\"]\n",
    "actual.valid <- data.valid[,'Choice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sad attempt at mlogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# method, tuneGrid, metric\n",
    "\n",
    "## preprocess data for mlogit model\n",
    "mnlogit.preprocess <- function(train, valid, test) {\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "  # list to store processed dataframes\n",
    "  out <- list()\n",
    "  \n",
    "  # process training set\n",
    "  M.train <- mlogit.data(train,\n",
    "                         shape = \"wide\",\n",
    "                         choice = \"Choice\",\n",
    "                         sep = \"\",\n",
    "                         varying = c(4:83),\n",
    "                         alt.levels = c(\"Ch1\", \"Ch2\", \"Ch3\", \"Ch4\"),\n",
    "                         id.var = \"Case\")\n",
    "  out$train <- M.train\n",
    "  \n",
    "  # process training set\n",
    "  M.valid <- mlogit.data(valid,\n",
    "                         shape = \"wide\",\n",
    "                         choice = \"Choice\",\n",
    "                         sep = \"\",\n",
    "                         varying = c(4:83),\n",
    "                         alt.levels = c(\"Ch1\", \"Ch2\", \"Ch3\", \"Ch4\"),\n",
    "                         id.var = \"Case\")\n",
    "  out$valid <- M.valid\n",
    "  \n",
    "  # process training set\n",
    "  M.test <- mlogit.data(test,\n",
    "                        shape = \"wide\",\n",
    "                        choice = \"Choice\",\n",
    "                        sep = \"\",\n",
    "                        varying = c(4:83),\n",
    "                        alt.levels = c(\"Ch1\", \"Ch2\", \"Ch3\", \"Ch4\"),\n",
    "                        id.var = \"Case\")\n",
    "  out$test <- M.test\n",
    "  \n",
    "  # return processed dataframes\n",
    "  return(out)\n",
    "}\n",
    "\n",
    "## fit mlogit model (with cross-validation)\n",
    "mnlogit.fit <- function(data) {\n",
    "  \n",
    "  # config parallel processing\n",
    "  #cluster <- makeCluster(detectCores() - 1)\n",
    "  \n",
    "  # initialise inner-model to use for cross-validation\n",
    "  control <- trainControl(method=\"cv\",\n",
    "                          number=10,\n",
    "                          savePredictions=\"final\",\n",
    "                          summaryFunction=mnLogLoss,\n",
    "                          classProbs=T,\n",
    "                          allowParallel=T)\n",
    "  \n",
    "  # specify parameter grid to compute over\n",
    "  grid <- expand.grid(cp=seq(0, 0.002, 0.0001)) \n",
    "  \n",
    "  # train the model (WARNING: LONG COMPUTATION TIME)\n",
    "  #registerDoParallel(cluster)\n",
    "#   M <- mlogit(Choice~NS+GN+FA+LD+BZ+FP+RP+PP+KA+SC+TS+MA+LB+HU+Price-1 | \n",
    "#                    segment+night+region+miles+age+income+ppark,\n",
    "#                    R=100,\n",
    "#                    panel=F,\n",
    "#                    data=data)\n",
    "  # loss_train (mlogit):  1.164466 \n",
    "  # loss_valid (mlogit):  1.129114 \n",
    "  M <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FP+RP+PP+KA+SC+TS+MA+LB+HU+Price-1 | \n",
    "              segment+night+region+miles+age | \n",
    "              0,\n",
    "              rpar=c(CC='n', GN='n', FP='n', RP='n', PP='n', LB='n', Price='t'),\n",
    "              #reflevel = \"Ch1\",\n",
    "              R = 100,\n",
    "              #halton = NA,\n",
    "              #correlation = T,\n",
    "              data = data,\n",
    "              panel = T,\n",
    "              print.level = F)\n",
    "  # loss_train (mlogit):  1.19749 \n",
    "  # loss_valid (mlogit):  1.153899 \n",
    "  #stopCluster(cluster)\n",
    "    \n",
    "  #registerDoSEQ()\n",
    "  \n",
    "  # return fitted model\n",
    "  return(M)\n",
    "}\n",
    "\n",
    "## predict using fitted mlogit model\n",
    "mnlogit.predict <- function(M, D, prob) {\n",
    "  \n",
    "  # get prediction accuracy on training set\n",
    "  P.train <- predict(M, D$train)\n",
    "  C.train <- apply(P.train, 1, which.max)\n",
    "  T.train <- table(C.train, actual.train) # confusion matrix, not really needed here\n",
    "  # sum(diag(T2.train))/NROW(data.train)\n",
    "  cat(\"loss_train (mlogit): \", logLoss(P.train, actual.train), \"\\n\")\n",
    "  C.train <- ifelse(C.train==1, \"Ch1\",\n",
    "                ifelse(C.train==2, \"Ch2\",\n",
    "                       ifelse(C.train==3, \"Ch3\",\n",
    "                              ifelse(C.train==4, \"Ch4\", 0))))\n",
    "  \n",
    "  # get prediction accuracy on validation set\n",
    "  P.valid <- predict(M, D$valid)\n",
    "  C.valid <- apply(P.valid, 1, which.max)\n",
    "  T.valid <- table(C.valid, actual.valid) # confusion matrix, not really needed here\n",
    "  # sum(diag(T2.valid))/NROW(data.valid)\n",
    "  cat(\"loss_valid (mlogit): \", logLoss(P.valid, actual.valid), \"\\n\")\n",
    "  \n",
    "  # get prediction on test set\n",
    "  D$test[,c(\"Ch1\", \"Ch2\", \"Ch3\", \"Ch4\")] <- 1\n",
    "  D$test[,c(\"Choice\")] <- T\n",
    "  P.test <- predict(M, D$test)\n",
    "  C.test <- apply(P.test, 1, which.max)\n",
    "  \n",
    "  # depending on `prob` input parameter, return either the probabilites or the choice predicted\n",
    "  if (prob) {\n",
    "    return(P.test)\n",
    "  } else {\n",
    "    return(C.train)\n",
    "  }\n",
    "}\n",
    "\n",
    "## wrapper function\n",
    "mnlogit <- function(prob=T) {\n",
    "  D <- mnlogit.preprocess(data.train, data.valid, data.test)\n",
    "  M <- mnlogit.fit(D$train)\n",
    "  return(mnlogit.predict(M, D, prob))\n",
    "}\n",
    "\n",
    "\n",
    "## DEBUGGING\n",
    "D <- mnlogit.preprocess(data.train, data.valid, data.test)\n",
    "M <- mnlogit.fit(D$train)\n",
    "P <- mnlogit.predict(M, D, prob=T)\n",
    "# loss_train (mlogit):  1.181399 \n",
    "# loss_valid (mlogit):  1.151575 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sad attempt at using CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# method, tuneGrid, metric\n",
    "\n",
    "## fit CART model (with cross-validation)\n",
    "cart.fit <- function(x, y) {\n",
    "    \n",
    "    # config parallel processing\n",
    "    cluster <- makeCluster(detectCores() - 1)\n",
    "    \n",
    "    # initialise CART inner-model to use for cross-validation\n",
    "    control <- trainControl(method=\"cv\",\n",
    "                            number=10,\n",
    "                            savePredictions=\"final\",\n",
    "                            summaryFunction=mnLogLoss,\n",
    "                            classProbs=T,\n",
    "                            allowParallel=T)\n",
    "\n",
    "    # specify parameter grid to compute over (e.g. complexity parameter for CART)\n",
    "    grid <- expand.grid(cp=seq(0, 0.001, 0.00005)) \n",
    "\n",
    "    # train the model (WARNING: LONG COMPUTATION TIME)\n",
    "    registerDoParallel(cluster)\n",
    "    #set.seed(1)\n",
    "    M <- train(x=x, y=y,\n",
    "                trControl=control,\n",
    "                tuneGrid=grid,\n",
    "                method=\"rpart\",\n",
    "                metric=\"logLoss\",\n",
    "                preProc=c(\"center\", \"scale\"))\n",
    "    stopCluster(cluster)\n",
    "    registerDoSEQ()\n",
    "    \n",
    "    # return fitted model\n",
    "    return(M)\n",
    "}\n",
    "\n",
    "## predict using fitted CART model\n",
    "cart.predict <- function(M, prob=T) {\n",
    "\n",
    "    # get prediction accuracy on training set\n",
    "    P.train <- predict(M, data.train, type=\"prob\")\n",
    "    C.train <- predict(M, data.train, type=\"raw\")\n",
    "    T.train <- table(C.train, actual.train) # confusion matrix, not really needed here\n",
    "    # sum(diag(T2.train))/NROW(data.train)\n",
    "    cat(\"loss_train (CART): \", logLoss(P.train, actual.train), \"\\n\")\n",
    "\n",
    "    # get prediction accuracy on validation set\n",
    "    P.valid <- predict(M, data.valid, type=\"prob\")\n",
    "    C.valid <- predict(M, data.valid, type=\"raw\")\n",
    "    T.valid <- table(C.valid, actual.valid) # confusion matrix, not really needed here\n",
    "    # sum(diag(T2.valid))/NROW(data.valid)\n",
    "    cat(\"loss_valid (CART): \", logLoss(P.valid, actual.valid), \"\\n\")\n",
    "\n",
    "    # get prediction on test set\n",
    "    P.test <- predict(M, data.test, type=\"prob\")\n",
    "    C.test <- predict(M, data.test, type=\"raw\")\n",
    "    \n",
    "    # depending on `prob` input parameter, return either the probabilites or the choice predicted\n",
    "    if (prob) {\n",
    "        return(P.test)\n",
    "    } else {\n",
    "        return(C.train)\n",
    "    }\n",
    "}\n",
    "\n",
    "## wrapper function\n",
    "cart <- function(x=data.train[,c(4:81, 83:94)], y=data.train[,c(99)], prob=T) {\n",
    "    M <- cart.fit(x, y)\n",
    "    return(cart.predict(M, prob))\n",
    "}\n",
    "\n",
    "## DEBUGGING\n",
    "# M2 <- cart.fit(x=data.train[,c(4:81, 83:94)], y=data.train[,c(99)])\n",
    "# P2 <- cart.predict(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualising cp vs logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw plot to see how the `mnLogLoss` changes over the various `cp` values\n",
    "options(repr.plot.width=9, repr.plot.height=4) # resize IRkernel plot size\n",
    "plot(M2$results$cp, M2$results$logLoss,\n",
    "     xlab=\"cp\",\n",
    "     ylab=\"mnLogLoss\")\n",
    "\n",
    "# get (index, cp) that gives lowest `mnLogLoss`\n",
    "M2$bestTune\n",
    "\n",
    "\n",
    "\n",
    "# options(repr.plot.width=9, repr.plot.height=7) # resize IRkernel plot size\n",
    "# prp(M2$finalModel, extra=4, type=4, branch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sad attempt at using random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## fit randomforest model (need cross-validation or not??? i dunno)\n",
    "## (clement) random forest model itself is a form of cross validation isnt it? so no need to do CV\n",
    "rf.fit <- function(x, y) {\n",
    "    \n",
    "    # config parallel processing\n",
    "    cluster <- makeCluster(35)\n",
    "    \n",
    "    # initialise rf inner-model to use for cross-validation\n",
    "    control <- trainControl(method=\"cv\",\n",
    "                            number=10,\n",
    "                            savePredictions=\"final\",\n",
    "                            summaryFunction=mnLogLoss,\n",
    "                            classProbs=T,\n",
    "                            allowParallel=T)\n",
    "    \n",
    "    # specify parameter grid to compute over\n",
    "    grid <- expand.grid(mtry=seq(10, 20, 1)) \n",
    "\n",
    "    # tune `mtry` variable\n",
    "    registerDoParallel(cluster)\n",
    "    #M <- tuneRF(x, y, stepFactor=1.2, improve=1e-5, ntreeTry=2000, doBest=T, mtryStart=10)\n",
    "    M <- train(x=x, y=y,\n",
    "                 trControl=control,\n",
    "                 tuneGrid=grid,\n",
    "                 method=\"rf\",\n",
    "                 nodesize=25,\n",
    "                 metric=\"logLoss\",\n",
    "                 ntree=2000,\n",
    "                 preProc=c(\"center\", \"scale\"))\n",
    "#     M <- randomForest(x, y, ntree=1000)\n",
    "    stopCluster(cluster)\n",
    "    registerDoSEQ()\n",
    "    \n",
    "    # return model with best-fitted `mtry` variable\n",
    "    return(M)\n",
    "}\n",
    "\n",
    "## predict using fitted randomforest model\n",
    "rf.predict <- function(M, prob) {\n",
    "    \n",
    "    # get prediction accuracy on train set\n",
    "    P.train <- predict(M, data.train, type=\"prob\")\n",
    "    C.train <- predict(M, data.train)\n",
    "    T.train <- table(C.train, actual.train) # confusion matrix, not really needed here\n",
    "    cat(\"acc_train (randomforest): \", sum(diag(T.train))/NROW(data.train), \"\\n\")\n",
    "    cat(\"loss_train (randomForest): \", logLoss(P.train, actual.train), \"\\n\")\n",
    "\n",
    "    # # get prediction accuracy on validation set\n",
    "    P.valid <- predict(M, data.valid, type=\"prob\")\n",
    "    C.valid <- predict(M, data.valid)\n",
    "    T.valid <- table(C.valid, actual.valid) # confusion matrix, not really needed here\n",
    "    cat(\"acc_valid (randomforest): \", sum(diag(T.valid))/NROW(data.valid), \"\\n\")\n",
    "    cat(\"loss_valid (randomForest): \", logLoss(P.valid, actual.valid), \"\\n\\n\")\n",
    "\n",
    "    # get prediction accuracy on test set\n",
    "    P.test <- predict(M, data.test, type=\"prob\")\n",
    "    C.test <- predict(M, data.test)\n",
    "\n",
    "    # depending on `prob` input parameter, return either the probabilites or the choice predicted\n",
    "    if (prob) {\n",
    "        return(P.test)\n",
    "    } else {\n",
    "        return(C.train)\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "## wrapper function for randomforest\n",
    "rf <- function(x=data.train[,c(4:63, 84:94)], y=data.train[,c(99)], prob=T) {\n",
    "    M <- rf.fit(x, y)\n",
    "    return(rf.predict(M, prob))\n",
    "}\n",
    "\n",
    "\n",
    "## DEBUGGING\n",
    "## [MODEL_OR_PREDICT].[DATASET_SIZE].[SPLIT_FACTOR].[#_OF_FACTORS]\n",
    "# M.final <- rf.fit(x=data.train.small[,c(4:63, 84:94)], y=data.train.small[,c(99)])\n",
    "# M.final$bestTune\n",
    "# P.final <- rf.predict(M.final, T)\n",
    "\n",
    "\n",
    "\n",
    "# write.csv(P.large.task.allBut4, paste(\"RF\", date()), row.names = F, col.names=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## SPLITTING BY TASK, EXCLUDE ALTERNATIVE 4 VARS\n",
    "M.large.task.allBut4 <- rf.fit(x=data.train[,c(4:63, 84:94)], y=data.train[,c(\"Choice\")])\n",
    "P.large.task.allBut4 <- rf.predict(M.large.task.allBut4, prob=T)\n",
    "# str(M.small.task.allBut4$finalModel)\n",
    "# options(repr.plot.width=8, repr.plot.height=15) # resize IRkernel plot size\n",
    "# vu <- varUsed(M.small.task.allBut4$finalModel, count=T)\n",
    "# vuSorted <- sort(vu, decreasing=F, index.return=T)\n",
    "# get number of times variable was used\n",
    "# dotchart(vuSorted$x, names(M.small.task.allBut4$finalModel$forest$xlevels[vuSorted$ix]))\n",
    "# get importance of variable in reducing impurity\n",
    "# varImpPlot(M.small.task.allBut4$finalModel, n.var=nrow(M.small.task.allBut4$finalModel$importance))\n",
    "# acc_train (randomforest):  1 \n",
    "# loss_train (randomForest):  0.2860298 \n",
    "# acc_valid (randomforest):  0.548381 \n",
    "# loss_valid (randomForest):  1.144018\n",
    "\n",
    "## SPLITTING BY TASK, INCLUDING ALL FACTORS\n",
    "# M.small.task.all <- rf.fit(x=data.train.small[,c(4:94)], y=data.train.small[,c(\"Choice\")])\n",
    "# P.small.task.all <- rf.predict(M.small.task.all, prob=T)\n",
    "# str(M.small.task.all$finalModel)\n",
    "# options(repr.plot.width=8, repr.plot.height=15) # resize IRkernel plot size\n",
    "# vu <- varUsed(M.small.task.all$finalModel, count=T)\n",
    "# vuSorted <- sort(vu, decreasing=F, index.return=T)\n",
    "# get number of times variable was used\n",
    "# dotchart(vuSorted$x, names(M.small.task.all$finalModel$forest$xlevels[vuSorted$ix]))\n",
    "# get importance of variable in reducing impurity\n",
    "# varImpPlot(M.small.task.all$finalModel, n.var=nrow(M.small.task.all$finalModel$importance))\n",
    "# acc_train (randomforest):  1 \n",
    "# loss_train (randomForest):  0.2860298 \n",
    "# acc_valid (randomforest):  0.548381 \n",
    "# loss_valid (randomForest):  1.144018\n",
    "\n",
    "# M.large.task.all <- rf.fit(x=data.train[,c(4:94)], y=data.train[,c(\"Choice\")])\n",
    "# P.large.task.all <- rf.predict(M.large.task.all, prob=T)\n",
    "# acc_train (randomforest):  1 \n",
    "# loss_train (randomForest):  0.2769319 \n",
    "# acc_valid (randomforest):  1 \n",
    "# loss_valid (randomForest):  0.2720686 \n",
    "\n",
    "## SPLITTING BY CASE\n",
    "# M.small.case.all <- rf.fit(x=data.train.small[,c(4:94)], y=data.train.small[,c(\"Choice\")])\n",
    "# P.small.case.all <- rf.predict.small(M.small.case.all, prob=T)\n",
    "# acc_train (randomforest):  1 \n",
    "# loss_train (randomForest):  0.2794337 \n",
    "# acc_valid (randomforest):  0.5024561 \n",
    "# loss_valid (randomForest):  1.197454 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_train (randomforest):  0.8123509 \n",
      "loss_train (randomForest):  0.6427379 \n",
      "acc_valid (randomforest):  0.8315789 \n",
      "loss_valid (randomForest):  0.615116 \n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in acc_train(randomforest): could not find function \"acc_train\"\n",
     "output_type": "error",
     "traceback": [
      "Error in acc_train(randomforest): could not find function \"acc_train\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# M.rf <- randomForest(x=data.train[,c(\"income\", \"miles\", \"year\", \"night\",\n",
    "#                                      \"Price1\", \"Price2\", \"Price3\", \"segment\",\n",
    "#                                      \"region\", \"age\", \"ppark\", \"educ\",\n",
    "#                                      \"BU1\", \"BU2\", \"BU3\", \"NS1\",\n",
    "#                                      \"NS2\", \"NS3\")],\n",
    "#                      y=data.train[,c(\"Choice\")],\n",
    "#                      ntree=1500,\n",
    "#                      mtry=)\n",
    "\n",
    "# cluster <- makeCluster(35)\n",
    "# registerDoParallel(cluster)\n",
    "\n",
    "# M.10.25 <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small, nodesize=25, ntree=2000, mtry = 10)\n",
    "\n",
    "# M.10.45 <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small, nodesize=45, ntree=2000, mtry = 10)\n",
    "\n",
    "# M.15.25 <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small, nodesize=25, ntree=2000, mtry = 15)\n",
    "\n",
    "\n",
    "# M.15.25.mod <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small.mod, nodesize=25, ntree=2000, mtry = 15)\n",
    "\n",
    "\n",
    "M.mod <- randomForest(as.factor(Choice) ~ income + segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "                       +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "                       +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "                       +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "                       , data=data.train.mod, nodesize=25, ntree=2000, mtry = 15)\n",
    "\n",
    "# M.15.45 <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small, nodesize=45, ntree=2000, mtry = 15)\n",
    "\n",
    "# M.15.28 <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small, nodesize=28, ntree=2000, mtry = 15)\n",
    "\n",
    "# M.15.25.full <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train, nodesize=25, ntree=2000, mtry = 15)\n",
    "\n",
    "# M.15.45 <- randomForest(as.factor(Choice) ~ segment + year + miles + night + gender + age + educ + region + Urb + ppark\n",
    "#                        +CC1+GN1+NS1+BU1+FA1+LD1+BZ1+FC1+FP1+RP1+PP1+KA1+SC1+TS1+NV1+MA1+LB1+AF1+HU1+Price1\n",
    "#                        +CC2+GN2+NS2+BU2+FA2+LD2+BZ2+FC2+FP2+RP2+PP2+KA2+SC2+TS2+NV2+MA2+LB2+AF2+HU2+Price2\n",
    "#                        +CC3+GN3+NS3+BU3+FA3+LD3+BZ3+FC3+FP3+RP3+PP3+KA3+SC3+TS3+NV3+MA3+LB3+AF3+HU3+Price3\n",
    "#                        , data=data.train.small, nodesize=45, ntree=2000, mtry = 15)\n",
    "\n",
    "# cluster <- makeCluster(35)\n",
    "# registerDoParallel(cluster)\n",
    "# M.tune <- tuneRF(data.train.small[,c(84:92, 94)], data.train.small[,c(\"Choice\")],\n",
    "#                  stepFactor=1.2, improve=1e-5, ntreeTry=1500, doBest=T, mtryStart=10)\n",
    "# stopCluster(cluster)\n",
    "# registerDoSEQ()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# P.10.25 <- rf.predict(M.10.25, prob=T)\n",
    "# P.10.45 <- rf.predict(M.10.45, prob=T)\n",
    "# P.15.25 <- rf.predict(M.15.25, prob=T)\n",
    "# P.15.25.mod <- rf.predict(M.15.25.mod, prob=T)\n",
    "# P.15.45 <- rf.predict(M.15.45, prob=T)\n",
    "\n",
    "P.mod <- rf.predict(M.mod, prob=T)\n",
    "\n",
    "\n",
    "## 25/10/small\n",
    "# acc_train (randomforest):  0.695335 \n",
    "# loss_train (randomForest):  0.8608042 \n",
    "# acc_valid (randomforest):  0.5005254 \n",
    "# loss_valid (randomForest):  1.165004 \n",
    "## 45/10/small\n",
    "# acc_train (randomforest):  0.6129078 \n",
    "# loss_train (randomForest):  0.9432376 \n",
    "# acc_valid (randomforest):  0.5022767 \n",
    "# loss_valid (randomForest):  1.16119 \n",
    "## 60/10/small\n",
    "# acc_train (randomforest):  0.5833743 \n",
    "# loss_train (randomForest):  0.9826437 \n",
    "# acc_valid (randomforest):  0.5015762 \n",
    "# loss_valid (randomForest):  1.163753 \n",
    "\n",
    "\n",
    "acc_train (randomforest):  0.6842105 \n",
    "loss_train (randomForest):  0.8392076 \n",
    "acc_valid (randomforest):  0.4975439 \n",
    "loss_valid (randomForest):  1.157477 \n",
    "\n",
    "acc_train (randomforest):  0.7370526 \n",
    "loss_train (randomForest):  0.765638 \n",
    "acc_valid (randomforest):  0.4978947 \n",
    "loss_valid (randomForest):  1.157089 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.csv(P.15.25.full, \"final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=15) # resize IRkernel plot size\n",
    "# vu <- varUsed(M.10.inc, count=T)\n",
    "# vuSorted <- sort(vu, decreasing=F, index.return=T)\n",
    "# # get number of times variable was used\n",
    "# dotchart(vuSorted$x, names(M.rf$forest$xlevels[vuSorted$ix]))\n",
    "# # get importance of variable in reducing impurity\n",
    "varImpPlot(M.mod, n.var=nrow(M.mod$importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fn <- getModelInfo(\"rfRules\")[[1]]$grid\n",
    "# str(data.train[,c(4:63, 84:94)])\n",
    "param <- getModelInfo(\"rf\")[[1]]$parameters\n",
    "param <- rbind(param, data.frame(parameter=\"nodesize\", class=\"numeric\", label=\"Nodesize\"))\n",
    "fn <- function (x, y, len = NULL, search = \"grid\") {\n",
    "    \n",
    "    if (search == \"grid\") {\n",
    "        out <- data.frame(mtry = caret::var_seq(p = ncol(x), \n",
    "            classification = is.factor(y), len = len), nodesize = seq(20, 60, 5))\n",
    "    }\n",
    "    else {\n",
    "        out <- data.frame(mtry = sample(1:ncol(x), size = len, \n",
    "            replace = TRUE), nodesize = sample(1:15, size = len, \n",
    "            replace = TRUE))\n",
    "    }\n",
    "    out[!duplicated(out), ]\n",
    "}\n",
    "\n",
    "rfCustom <- getModelInfo(\"rf\")[[1]]\n",
    "rfCustom$grid <- fn\n",
    "rfCustom$parameters <- param\n",
    "# rfCustom$fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>20</li>\n",
       "\t<li>25</li>\n",
       "\t<li>30</li>\n",
       "\t<li>35</li>\n",
       "\t<li>40</li>\n",
       "\t<li>45</li>\n",
       "\t<li>50</li>\n",
       "\t<li>55</li>\n",
       "\t<li>60</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 20\n",
       "\\item 25\n",
       "\\item 30\n",
       "\\item 35\n",
       "\\item 40\n",
       "\\item 45\n",
       "\\item 50\n",
       "\\item 55\n",
       "\\item 60\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 20\n",
       "2. 25\n",
       "3. 30\n",
       "4. 35\n",
       "5. 40\n",
       "6. 45\n",
       "7. 50\n",
       "8. 55\n",
       "9. 60\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 20 25 30 35 40 45 50 55 60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq(20, 60, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training <- data.train[,c(\"CC1\", \"CC2\", \"CC3\", \"CC4\",\n",
    "              \"FP1\", \"FP2\", \"FP3\", \"FP4\",\n",
    "              \"RP1\", \"RP2\", \"RP3\", \"RP4\",\n",
    "              \"PP1\", \"PP2\", \"PP3\", \"PP4\",\n",
    "              \"KA1\", \"KA2\", \"KA3\", \"KA4\",\n",
    "              \"SC1\", \"SC2\", \"SC3\", \"SC4\",\n",
    "              \"LB1\", \"LB2\", \"LB3\", \"LB4\",\n",
    "              \"segment\", \"night\", \"miles\", \"region\", \"ppark\")]\n",
    "valid <- data.valid[,c(\"CC1\", \"CC2\", \"CC3\", \"CC4\",\n",
    "              \"FP1\", \"FP2\", \"FP3\", \"FP4\",\n",
    "              \"RP1\", \"RP2\", \"RP3\", \"RP4\",\n",
    "              \"PP1\", \"PP2\", \"PP3\", \"PP4\",\n",
    "              \"KA1\", \"KA2\", \"KA3\", \"KA4\",\n",
    "              \"SC1\", \"SC2\", \"SC3\", \"SC4\",\n",
    "              \"LB1\", \"LB2\", \"LB3\", \"LB4\",\n",
    "              \"segment\", \"night\", \"miles\", \"region\", \"ppark\")]\n",
    "test <- data.test[,c(\"CC1\", \"CC2\", \"CC3\", \"CC4\",\n",
    "              \"FP1\", \"FP2\", \"FP3\", \"FP4\",\n",
    "              \"RP1\", \"RP2\", \"RP3\", \"RP4\",\n",
    "              \"PP1\", \"PP2\", \"PP3\", \"PP4\",\n",
    "              \"KA1\", \"KA2\", \"KA3\", \"KA4\",\n",
    "              \"SC1\", \"SC2\", \"SC3\", \"SC4\",\n",
    "              \"LB1\", \"LB2\", \"LB3\", \"LB4\",\n",
    "              \"segment\", \"night\", \"miles\", \"region\", \"ppark\")]\n",
    "training.small <- data.train.small[,c(\"CC1\", \"CC2\", \"CC3\", \"CC4\",\n",
    "              \"FP1\", \"FP2\", \"FP3\", \"FP4\",\n",
    "              \"RP1\", \"RP2\", \"RP3\", \"RP4\",\n",
    "              \"PP1\", \"PP2\", \"PP3\", \"PP4\",\n",
    "              \"KA1\", \"KA2\", \"KA3\", \"KA4\",\n",
    "              \"SC1\", \"SC2\", \"SC3\", \"SC4\",\n",
    "              \"LB1\", \"LB2\", \"LB3\", \"LB4\",\n",
    "              \"segment\", \"night\", \"miles\", \"region\", \"ppark\")]\n",
    "\n",
    "# P.small.task.all <- rf.predict.small(M.small.task.all, prob=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.predict.small <- function(M, prob) {\n",
    "    \n",
    "    # get prediction accuracy on train set\n",
    "    P.train <- predict(M, data.train.small, type=\"prob\")\n",
    "    C.train <- predict(M, data.train.small)\n",
    "    T.train <- table(C.train, actual.train.small) # confusion matrix, not really needed here\n",
    "    cat(\"acc_train (randomforest): \", sum(diag(T.train))/NROW(data.train.small), \"\\n\")\n",
    "    cat(\"loss_train (randomForest): \", logLoss(P.train, actual.train.small), \"\\n\")\n",
    "\n",
    "    # # get prediction accuracy on validation set\n",
    "    P.valid <- predict(M, data.valid, type=\"prob\")\n",
    "    C.valid <- predict(M, data.valid)\n",
    "    T.valid <- table(C.valid, actual.valid) # confusion matrix, not really needed here\n",
    "    cat(\"acc_valid (randomforest): \", sum(diag(T.valid))/NROW(data.valid), \"\\n\")\n",
    "    cat(\"loss_valid (randomForest): \", logLoss(P.valid, actual.valid), \"\\n\")\n",
    "\n",
    "    # get prediction accuracy on test set\n",
    "    P.test <- predict(M, data.test, type=\"prob\")\n",
    "    C.test <- predict(M, data.test)\n",
    "\n",
    "    # depending on `prob` input parameter, return either the probabilites or the choice predicted\n",
    "    if (prob) {\n",
    "        return(P.test)\n",
    "    } else {\n",
    "        return(C.train)\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# P.small <- rf.predict.small(M.small, prob=T)\n",
    "\n",
    "# acc_train (randomforest):  1 \n",
    "# loss_train (randomForest):  0.2860298 \n",
    "# acc_valid (randomforest):  0.548381 \n",
    "# loss_valid (randomForest):  1.144018 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sad attempt at using gradient-boosting trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## preprocess data for mlogit model\n",
    "xgb.preprocess <- function(train, valid, test) {\n",
    "    \n",
    "    # list to store processed dataframes\n",
    "    out <- list()\n",
    "    \n",
    "    # setting dataframes as datatables\n",
    "    table.train <- as.data.table(train)\n",
    "    table.valid <- as.data.table(valid)\n",
    "    table.test <- as.data.table(test)\n",
    "\n",
    "    # rename last col to numbers\n",
    "    nameLastCol <- names(train)[ncol(train)]\n",
    "    y.train <- table.train[, nameLastCol, with = F][[1]] %>% gsub('Ch', '', .) %>% {as.integer(.)-1}\n",
    "    y.valid <- table.train[, nameLastCol, with = F][[1]] %>% gsub('Ch', '', .) %>% {as.integer(.)-1}\n",
    "    y.test <- table.train[, nameLastCol, with = F][[1]] %>% gsub('Ch', '', .) %>% {as.integer(.)-1}\n",
    "    out$y.train <- y.train\n",
    "    out$y.valid <- y.valid\n",
    "    out$y.test <- y.test\n",
    "#     out$y.train <- table.train[,99]\n",
    "#     out$y.valid <- table.valid[,99]\n",
    "#     out$y.test <- table.test[,99]\n",
    "\n",
    "    # preprocessing training dataset\n",
    "#     table.train <- table.train[,c(4:94)]\n",
    "    table.train <- table.train[,c(4:84, 86:94)]\n",
    "#     table.train <- as.matrix(table.train)\n",
    "    table.train <- table.train[,lapply(.SD,as.numeric)] %>% as.matrix\n",
    "    out$table.train <- table.train\n",
    "\n",
    "    # preprocessing validation dataset\n",
    "#     table.valid <- table.valid[,c(4:94)]\n",
    "    table.valid <- table.valid[,c(4:84, 86:94)]\n",
    "#     table.valid <- as.matrix(table.valid)\n",
    "    table.valid <- table.valid[,lapply(.SD,as.numeric)] %>% as.matrix\n",
    "    out$table.valid <- table.valid\n",
    "\n",
    "    # preprocessing test dataset\n",
    "#     table.test <- table.test[,c(4:94)]\n",
    "    table.test <- table.test[,c(4:84, 86:94)]\n",
    "#     table.test <- as.matrix(table.test)\n",
    "    table.test <- table.test[,lapply(.SD,as.numeric)] %>% as.matrix\n",
    "    out$table.test <- table.test\n",
    "    \n",
    "    # return processed data\n",
    "    return(out)\n",
    "    \n",
    "}\n",
    "\n",
    "## fit gradient boosting model (with cross-validation)\n",
    "xgb.fit <- function(D) {\n",
    "    \n",
    "    # config parallel processing\n",
    "    cluster <- makeCluster(4 - 1)\n",
    "    \n",
    "    xgbCust <- getModelInfo(\"xgbTree\")$xgbTree\n",
    "    params <- data.frame(parameter=c(\"objective\",\"eval_metric\",\"num_class\"),\n",
    "                     class=c(\"character\",\"character\",\"numeric\"),\n",
    "                     label=c(\"Objective function\", \"Metric\", \"Classes\"))\n",
    "    names(params) <- c(\"parameter\", \"class\", \"label\")\n",
    "    xgbCust$parameters <- rbind(xgbCust$parameters, params)\n",
    "    \n",
    "    # initialise CART inner-model to use for cross-validation\n",
    "    control <- trainControl(method=\"cv\",\n",
    "                            number=10,\n",
    "                            #savePredictions=\"final\",\n",
    "                            #returnResamp=\"final\",\n",
    "                            #summaryFunction=mnLogLoss,\n",
    "                            #classProbs=T,\n",
    "                            allowParallel=T)\n",
    "\n",
    "    # specify parameter grid to compute over (e.g. complexity parameter for CART)\n",
    "#     grid <- expand.grid(nrounds=seq(10, 10, 1) ,\n",
    "#                         lambda=seq(0, 0, 1) ,\n",
    "#                         alpha=seq(0, 0, 1) ,\n",
    "#                         eta=seq(0.3, 0.3, 0.05),\n",
    "#                         objective=\"multi:softprob\",\n",
    "#                         eval_metric=\"mlogloss\",\n",
    "#                         num_class=4)\n",
    "    grid <- expand.grid(nrounds=seq(20, 50, 1),\n",
    "                        max_depth=seq(89, 89, 1),\n",
    "                        eta=seq(0.1, 0.4, 0.05),\n",
    "                        gamma=seq(0, 0, 1),\n",
    "                        colsample_bytree=seq(1, 1, 1),\n",
    "                        min_child_weight=seq(1, 1, 1),\n",
    "                        subsample=seq(0.7, 1, 0.1))\n",
    "#                         objective=\"multi:softprob\",\n",
    "#                         eval_metric=\"mlogloss\",\n",
    "#                         num_class=4)\n",
    "    \n",
    "    # train the model (WARNING: LONG COMPUTATION TIME)\n",
    "    registerDoParallel(cluster)\n",
    "#     M <- xgboost(param=param,\n",
    "#                  data=x,\n",
    "#                  label=y,\n",
    "#                  nrounds=cv.nround)\n",
    "    M <- train(x=D$table.train, y=D$y.train,\n",
    "               trControl=control,\n",
    "               tuneGrid=grid,\n",
    "               #method=\"xgbLinear\",\n",
    "               #method=xgbCust,\n",
    "               method=\"xgbTree\",\n",
    "               objective=\"multi:softprob\",\n",
    "               eval_metric=\"mlogloss\",\n",
    "               num_class=4,\n",
    "               #metric=\"logLoss\",\n",
    "               preProc=c(\"center\", \"scale\"))\n",
    "    stopCluster(cluster)\n",
    "    registerDoSEQ()\n",
    "    \n",
    "    # return fitted model\n",
    "    return(M)\n",
    "}\n",
    "\n",
    "## predict using fitted randomforest model\n",
    "xgb.predict <- function(M, D, prob) {\n",
    "    \n",
    "    # get prediction accuracy on train set\n",
    "    P.train <- matrix(predict(M, D$table.train), ncol = 4, byrow = T)\n",
    "    C.train <- max.col(P.train)\n",
    "    logLoss(P.train, actual.train)\n",
    "\n",
    "    # get prediction accuracy on validation set\n",
    "    P.valid <- matrix(predict(M, D$table.valid), ncol = 4, byrow = T)\n",
    "    C.valid <- max.col(P.train)\n",
    "    logLoss(P.valid, actual.valid)\n",
    "\n",
    "    # get prediction accuracy on test set\n",
    "    P.test <- matrix(predict(M, D$table.test), ncol = 4, byrow = T)\n",
    "    C.test <- max.col(P.train)\n",
    "    # logLoss(P4.valid, actual.valid)\n",
    "\n",
    "    # depending on `prob` input parameter, return either the probabilites or the choice predicted\n",
    "    if (prob) {\n",
    "        return(P.test)\n",
    "    } else {\n",
    "        return(C.train)\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "## wrapper function for randomforest\n",
    "xgb <- function(train=data.train, valid=data.valid, test=data.test,  prob=T) {\n",
    "    D <- xgb.preprocess(train, valid, test)\n",
    "    M <- xgb.fit(D)\n",
    "    return(xgb.predict(M, D, prob))\n",
    "}\n",
    "\n",
    "## DEBUGGING\n",
    "D <- xgb.preprocess(data.train, data.valid, data.test)\n",
    "# M <- suppressWarnings(xgb.fit(D))\n",
    "M <- xgb.fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numberOfClasses <-4\n",
    "\n",
    "param <- list(\"objective\" = \"multi:softprob\",\n",
    "              \"eval_metric\" = \"mlogloss\",\n",
    "              \"num_class\" = 4)\n",
    "#               \"max_depth\" = 6,\n",
    "#               \"eta\" = 0.3)\n",
    "\n",
    "cv.nround <- 47\n",
    "cv.nfold <- 10\n",
    "\n",
    "D <- xgb.preprocess(data.train, data.valid, data.test)\n",
    "# M4 <- xgboost(param=param, data=D$table.train, label=D$y.train, nrounds=cv.nround)\n",
    "M4 <- xgb.cv(param=param, data=D$table.train, label=D$y.train, nrounds=cv.nround, nfold=cv.nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get prediction accuracy on train set\n",
    "P4.train <- matrix(predict(M, D$table.train), ncol = 4, byrow = T)\n",
    "C4.train <- max.col(P4.train)\n",
    "T4.train <- cbind(C4.train, actual.train)\n",
    "NROW(subset(T4.train, T4.train[,1] == T4.train[,2]))/NROW(T4.train)\n",
    "logLoss(P4.train, actual.train)\n",
    "\n",
    "# get prediction accuracy on validation set\n",
    "P4.valid <- matrix(predict(M, D$table.valid), ncol = 4, byrow = T)\n",
    "C4.valid <- max.col(P4.valid)\n",
    "T4.valid <- cbind(C4.valid, actual.valid)\n",
    "NROW(subset(T4.valid, T4.valid[,1] == T4.valid[,2]))/NROW(T4.valid)\n",
    "logLoss(P4.valid, actual.valid)\n",
    "\n",
    "# get prediction accuracy on test set\n",
    "P4.test <- matrix(predict(M, D$table.test), ncol = 4, byrow = T)\n",
    "C4.test <- max.col(P4.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?createFolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experimental stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a pathetic attempt at selecting relevant features\n",
    "featureSelect1 <- function(data) {\n",
    "    \n",
    "    try.train <- data\n",
    "    # for (i in c(4:22, 24:42)) {\n",
    "    #     try.train[,i] <- ifelse(try.train[,i]>0, 1, 0)\n",
    "    # }\n",
    "\n",
    "    # try.train[,c(4:22)]\n",
    "    # try.train[,c(24:42)]\n",
    "    # try.train[,c(44:62)]\n",
    "    # try.train[,c(64:82)]\n",
    "\n",
    "    try.train$Alt1 <- rowSums(try.train[,c(4:22)])\n",
    "    try.train$Ratio1 <- try.train$Alt1/try.train$Price1\n",
    "    try.train$Alt2 <- rowSums(try.train[,c(24:42)])\n",
    "    try.train$Ratio2 <- try.train$Alt2/try.train$Price2\n",
    "    try.train$Alt3 <- rowSums(try.train[,c(44:62)])\n",
    "    try.train$Ratio3 <- try.train$Alt3/try.train$Price3\n",
    "    try.train$Alt4 <- rowSums(try.train[,c(64:82)])\n",
    "    try.train$Ratio4 <- 0\n",
    "    \n",
    "    return(try.train[,c(84, 86:90, 92:94, 101, 103, 105, 107, 99)])\n",
    "}\n",
    "\n",
    "# another pathetic attempt at selecting relevant features\n",
    "featureSelect2 <- function(data) {\n",
    "    \n",
    "    try.train <- data\n",
    "    for (i in c(4:22)) {\n",
    "        try.train[,i] <- try.train[,i]/try.train[23]\n",
    "    }\n",
    "    for (i in c(24:42)) {\n",
    "        try.train[,i] <- try.train[,i]/try.train[43]\n",
    "    }\n",
    "    for (i in c(44:62)) {\n",
    "        try.train[,i] <- try.train[,i]/try.train[63]\n",
    "    }\n",
    "    for (i in c(64:82)) {\n",
    "        try.train[,i] <- try.train[,i]/try.train[83]\n",
    "    }\n",
    "\n",
    "    # try.train[,c(4:22)]\n",
    "    # try.train[,c(24:42)]\n",
    "    # try.train[,c(44:62)]\n",
    "    # try.train[,c(64:82)]\n",
    "\n",
    "#     try.train$Alt1 <- rowSums(try.train[,c(4:22)])\n",
    "#     try.train$Ratio1 <- try.train$Alt1/try.train$Price1\n",
    "#     try.train$Alt2 <- rowSums(try.train[,c(24:42)])\n",
    "#     try.train$Ratio2 <- try.train$Alt2/try.train$Price2\n",
    "#     try.train$Alt3 <- rowSums(try.train[,c(44:62)])\n",
    "#     try.train$Ratio3 <- try.train$Alt3/try.train$Price3\n",
    "#     try.train$Alt4 <- rowSums(try.train[,c(64:82)])\n",
    "#     try.train$Ratio4 <- 0\n",
    "    \n",
    "    return(try.train[,c(84, 86:90, 92:94, 101, 103, 105, 107, 99)])\n",
    "}\n",
    "\n",
    "try.train <- featureSelect(data.train)\n",
    "try.valid <- featureSelect(data.valid)\n",
    "try.test <- featureSelect(data.test)\n",
    "str(try.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try.train.mlogit <- mlogit.data(try.train,\n",
    "                         shape = \"wide\",\n",
    "                         choice = \"Choice\")\n",
    "#                          sep = \"\",\n",
    "#                          varying = c(4:83),\n",
    "#                          alt.levels = c(\"Ch1\", \"Ch2\", \"Ch3\", \"Ch4\"),\n",
    "#                          id.var = \"Case\")\n",
    "\n",
    "# try.train.mlogit\n",
    "\n",
    "rf.expt <- randomForest(Choice~segment+miles+night+gender+age+educ+Urb+income+ppark+Ratio1+Ratio2+Ratio3+Ratio4,\n",
    "                        data=try.train,\n",
    "                        mtry=13,\n",
    "                        ntree=1000)\n",
    "\n",
    "# rf.expt <- tuneRF(try.train[,c(1:13)], try.train[,c(14)], stepFactor=1.1, improve=1e-5, ntree=1000, doBest=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.expt.p <- predict(rf.expt, try.valid, type=\"response\")\n",
    "pred <- cbind(rf.expt.p, try.valid[,\"Choice\"])\n",
    "# pred\n",
    "NROW(subset(pred, pred[,1]==pred[,2]))/NROW(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preprocessing to make all columns output same format choices\n",
    "M3.train.choice <- ifelse(C3.train==\"Ch1\", 1, ifelse(C3.train==\"Ch2\", 2, ifelse(C3.train==\"Ch3\", 3, ifelse(C3.train==\"Ch4\", 4, 0))))\n",
    "M3.valid.choice <- ifelse(C3.valid==\"Ch1\", 1, ifelse(C3.valid==\"Ch2\", 2, ifelse(C3.valid==\"Ch3\", 3, ifelse(C3.valid==\"Ch4\", 4, 0))))\n",
    "M3.test.choice <- ifelse(C3.test==\"Ch1\", 1, ifelse(C3.test==\"Ch2\", 2, ifelse(C3.test==\"Ch3\", 3, ifelse(C3.test==\"Ch4\", 4, 0))))\n",
    "\n",
    "# get meta dataframe for model stacking\n",
    "U1.train.features <- data.frame(M1=XGB.train.choices, M3=M3.train.choice)\n",
    "U1.train <- cbind(train, U1.train.features)\n",
    "U1.train <- U1.train[,c(\"Choice\", \"M1\", \"M3\")]\n",
    "# U1.train$Choice <- ifelse(U1.train$Choice==\"Ch1\", 1,\n",
    "#                           ifelse(U1.train$Choice==\"Ch2\", 2,\n",
    "#                                  ifelse(U1.train$Choice==\"Ch3\", 3,\n",
    "#                                         ifelse(U1.train$Choice==\"Ch4\", 4, 0))))\n",
    "\n",
    "U1.valid.features <- data.frame(M1=XGB.valid.choices, M3=M3.valid.choice)\n",
    "U1.valid <- cbind(valid, U1.valid.features)\n",
    "U1.valid <- U1.valid[,c(\"Choice\", \"M1\", \"M3\")]\n",
    "U1.valid$Choice <- ifelse(U1.valid$Choice==\"Ch1\", 1,\n",
    "                          ifelse(U1.valid$Choice==\"Ch2\", 2,\n",
    "                                 ifelse(U1.valid$Choice==\"Ch3\", 3,\n",
    "                                        ifelse(U1.valid$Choice==\"Ch4\", 4, 0))))\n",
    "\n",
    "U1.test.features <- data.frame(Choice=factor(1, levels=c(1,2,3,4)), M1=XGB.test.choices, M3=M3.test.choice)\n",
    "# U1.test <- cbind(factor(1, levels=c(1,2,3,4)), U1.test.features)\n",
    "# U1.test <- U1.test[,c(\"Choice\", \"M1\", \"M3\")]\n",
    "# U1.test$Choice <- ifelse(U1.test$Choice==\"Ch1\", 1,\n",
    "#                           ifelse(U1.test$Choice==\"Ch2\", 2,\n",
    "#                                  ifelse(U1.test$Choice==\"Ch3\", 3,\n",
    "#                                         ifelse(U1.test$Choice==\"Ch4\", 4, 0))))\n",
    "U1.test$Choice <- factor(1, levels=c(1,2,3,4))\n",
    "U1.test\n",
    "# str(U1.test)\n",
    "# str(U1.train)\n",
    "# subset(U1.test, is.na(U1.test)==T)\n",
    "# which(is.na(U1.test))\n",
    "# test[16398:16416,]\n",
    "\n",
    "# fit new model to U1, using [M1,M2,M3] as features\n",
    "library(LiblineaR)\n",
    "# ?LiblineaR\n",
    "U1 <- LiblineaR(data=U1.train[,2:3], target=U1.train[,1], type=6, cost=100)\n",
    "# U1 <- LiblineaR(data=U1.train[,c(2:4)], target=U1.train[,1], type=6, cost=100)\n",
    "# summary(U1)\n",
    "# UP1.valid <- predict(U1, U1.valid, proba=T)\n",
    "# UP1.test <- predict(U1, U1.test, proba=T)\n",
    "# UP1.valid\n",
    "# UP1.test\n",
    "# UP1.test$probabilities\n",
    "\n",
    "\n",
    "# initialise CART inner-model to use for cross-validation\n",
    "# control <- trainControl(method=\"cv\", number=10, savePredictions=\"final\")\n",
    "\n",
    "# # specify parameter grid to compute over (e.g. complexity parameter for CART)\n",
    "# grid <- expand.grid(cp=seq(0, 0.005, 0.0001)) \n",
    "\n",
    "# # train the model (WARNING: LONG COMPUTATION TIME)\n",
    "# set.seed(1)\n",
    "# M2 <- train(Choice~.-Ch1-Ch2-Ch3-Ch4-1, data=train, trControl=control, tuneGrid=grid, method=\"rpart\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
